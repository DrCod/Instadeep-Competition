{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "enzyme.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FNIXr4y_gsY",
        "outputId": "7c504818-9716-498f-d7e0-e85cc6c22d08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-6odQTIwrL",
        "outputId": "2257e5e4-152b-40e9-e207-5f34817560b4"
      },
      "source": [
        "import tensorflow\r\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OnMAmnWJBN5",
        "outputId": "2ba71b21-d98c-4c5d-903d-436b51143ab4"
      },
      "source": [
        "!python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2bCNWrh_rjI",
        "outputId": "8761c408-86d0-4258-f0da-34500547eb0c"
      },
      "source": [
        "!ls 'drive/My Drive/Enzyme'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs\tmodels\tnotebooks  source\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXrd8GC3IREZ"
      },
      "source": [
        "# Import Libraries\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0,'drive/My Drive/Enzyme')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import keras\n",
        "\n",
        "\n",
        "from numpy.random import seed\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Input,concatenate, Bidirectional, SpatialDropout1D, GlobalAveragePooling1D,Reshape,Conv2D, MaxPool2D\n",
        "from keras.layers import Add, Activation,LayerNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4524B4ggGfv",
        "outputId": "89bf8356-b682-40fb-fcbe-14656ada904d"
      },
      "source": [
        "# detect and init the TPU\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "# instantiate a distribution strategy\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.225.74:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.225.74:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVoBvByMRIwa"
      },
      "source": [
        "# build Tokenizer\n",
        "df = pd.read_csv('drive/My Drive/Enzyme/inputs/Train.csv')\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(df.SEQUENCE.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgEm729NHU2-"
      },
      "source": [
        "# Save Tokenizer\r\n",
        "with open('drive/MyDrive/Enzyme/models/tokenizerndr.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tktPyH9hOkii"
      },
      "source": [
        "# Fix seed\n",
        "seed(1)\n",
        "tf.random.set_seed(seed=2)\n",
        "\n",
        "EPOCHS = 30\n",
        "batch_size = 4096\n",
        "max_len = 300\n",
        "lr = 0.001\n",
        "\n",
        "# Training Function\n",
        "def train(tokenizer,fold_):\n",
        "\n",
        "    df = pd.read_csv('drive/My Drive/Enzyme/inputs/stratified_5_fold.csv')\n",
        "    # dataframes for train and val \n",
        "    \n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "\n",
        "    val_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "\n",
        "    \n",
        "    #tokenize train and val\n",
        "\n",
        "    xtrain = tokenizer.texts_to_sequences(train_df.SEQUENCE.values)\n",
        "    xval = tokenizer.texts_to_sequences(val_df.SEQUENCE.values)\n",
        "\n",
        "    # pad sequence\n",
        "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,padding='post',truncating='post',maxlen=max_len)\n",
        "    xval = tf.keras.preprocessing.sequence.pad_sequences(xval,padding='post',truncating='post',maxlen=max_len)\n",
        "\n",
        "    # extract labels from dataframes\n",
        "    ytrain = train_df.loc[:,['LABEL']].values\n",
        "    yval = val_df.loc[:,['LABEL']].values\n",
        "\n",
        "    # one hot encoding\n",
        "    ytrain_enc = np_utils.to_categorical(ytrain)\n",
        "    yval_enc = np_utils.to_categorical(yval)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # printing shapes of train and val \n",
        "    print(xtrain.shape)\n",
        "    print(xval.shape)\n",
        "\n",
        "    print(ytrain_enc.shape)\n",
        "    print(yval_enc.shape)\n",
        "\n",
        "    print(len(word_index))\n",
        "\n",
        "    # Define Model: Training on TPU\n",
        "    \n",
        "    with tpu_strategy.scope():\n",
        "        \n",
        "        sequence_input = Input(shape=(max_len,))\n",
        "\n",
        "        embedding_layer1 = Embedding(input_dim=len(word_index)+1 ,\n",
        "                                output_dim=128,\n",
        "                            input_length=max_len)\n",
        "\n",
        "        x_emb1 = embedding_layer1(sequence_input)\n",
        "        x = SpatialDropout1D(0.2)(x_emb1)\n",
        "\n",
        "        x = Bidirectional(GRU(300,return_sequences=True))(x)\n",
        "        x = Bidirectional(GRU(300,return_sequences=True))(x)\n",
        "        x = Conv1D(300, kernel_size = 7, padding = \"same\",activation='relu', kernel_initializer = \"he_uniform\")(x)\n",
        "        avg_pool1 = GlobalAveragePooling1D()(x)\n",
        "        max_pool1 = GlobalMaxPooling1D()(x)     \n",
        "        \n",
        "        x = concatenate([avg_pool1, max_pool1])\n",
        "        x = Dense(1024, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.8)(x)\n",
        "        preds = Dense(20, activation='softmax')(x)\n",
        "        \n",
        "        model = Model(sequence_input, preds)\n",
        "        \n",
        "\n",
        "        model.summary()\n",
        "      \n",
        "        model.compile(loss='categorical_crossentropy',steps_per_execution = 50,\n",
        "                      optimizer= tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "                      ,metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model with early stopping,save_best and reduce on plateau callbacks\n",
        "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
        "    save_best = ModelCheckpoint(f\"drive/My Drive/Enzyme/models/model_{fold_}.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
        "    rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='min')\n",
        "\n",
        "\n",
        "\n",
        "    model.fit(xtrain, y=ytrain_enc, batch_size=batch_size, epochs=EPOCHS, \n",
        "          verbose=1, validation_data=(xval, yval_enc), callbacks=[earlystop,save_best,rl])\n",
        "    \n",
        "    #return tokenizer\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBliXBPzxJny"
      },
      "source": [
        "# Load saved Tokenizer and start training\n",
        "with open('drive/MyDrive/Enzyme/models/tokenizerndr.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "for i in range(5):\n",
        "  train(tokenizer,fold_=i)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKXMxaMtGHPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e33d969-ff47-4ae7-92d5-343484131945"
      },
      "source": [
        "# Inference \r\n",
        "\r\n",
        "fold = 5\r\n",
        "num_classes = 20\r\n",
        "# Load Tokenizer\r\n",
        "with open('drive/MyDrive/Enzyme/models/tokenizerndr.pickle', 'rb') as handle:\r\n",
        "    tokenizer = pickle.load(handle)\r\n",
        "\r\n",
        "# Load and tokenize test data\r\n",
        "test_df = pd.read_csv('drive/MyDrive/Enzyme/inputs/Test.csv')\r\n",
        "\r\n",
        "xtest = tokenizer.texts_to_sequences(test_df.SEQUENCE.values)\r\n",
        "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,padding='post',truncating='post',maxlen=300)\r\n",
        "final_preds = []\r\n",
        "\r\n",
        "for i in range(fold):\r\n",
        "  loaded_model = tf.keras.models.load_model(f\"drive/My Drive/Enzyme/models/model_{i}.h5\")\r\n",
        "  preds = loaded_model.predict(xtest)\r\n",
        "  print(preds.shape)\r\n",
        "  final_preds.append(preds)\r\n",
        "\r\n",
        "\r\n",
        "   \r\n",
        "pred_0 = np.concatenate(final_preds,axis=0)\r\n",
        "print(pred_0.shape)\r\n",
        "n_pred = pred_0.reshape(fold,test_df.shape[0],num_classes)\r\n",
        "avg = np.mean(n_pred,axis=0)\r\n",
        "y_classes = np.argmax(avg,axis=1)\r\n",
        "print(y_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(253146, 20)\n",
            "(253146, 20)\n",
            "(253146, 20)\n",
            "(253146, 20)\n",
            "(253146, 20)\n",
            "(1265730, 20)\n",
            "[ 7  2  2 ... 19  0  7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U77qd5gJOMQo"
      },
      "source": [
        "# groud truth labels given by label_encoder\n",
        "'''\n",
        "ground = ['class0' 'class1' 'class10' 'class11' 'class12' 'class13' 'class14'\n",
        " 'class15' 'class16' 'class17' 'class18' 'class19' 'class2' 'class3'\n",
        " 'class4' 'class5' 'class6' 'class7' 'class8' 'class9']'''\n",
        "\n",
        "idx2class = {0:'class0',1:'class1',2:'class10',3:'class11',4:'class12',5:'class13',6:'class14',7:'class15',\n",
        "             8:'class16',9:'class17',10:'class18',11:'class19',12:'class2',13:'class3',\n",
        "             14:'class4',15:'class5',16:'class6',17:'class7',18:'class8',19:'class9'}\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub[\"SEQUENCE_ID\"] = test_df[\"SEQUENCE_ID\"]\n",
        "sub[\"LABEL\"] = y_classes\n",
        "sub['LABEL'].replace(idx2class, inplace=True)\n",
        "sub.head()\n",
        "sub.to_csv('drive/MyDrive/Enzyme/inputs/submission.csv', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9hDb6_bUSRi"
      },
      "source": [
        "sub = pd.read_csv('drive/MyDrive/Enzyme/inputs/subm.csv')\r\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1rr3eYUV_J"
      },
      "source": [
        "# Pseudo_labeling\r\n",
        "\r\n",
        "# We need to add the test sequences to the submission file in order to add it to our training data, then retrain the model\r\n",
        "\r\n",
        "'''\r\n",
        "ground = ['class0' 'class1' 'class10' 'class11' 'class12' 'class13' 'class14'\r\n",
        " 'class15' 'class16' 'class17' 'class18' 'class19' 'class2' 'class3'\r\n",
        " 'class4' 'class5' 'class6' 'class7' 'class8' 'class9']'''\r\n",
        "\r\n",
        "idx2class = {0:'class0',1:'class1',2:'class10',3:'class11',4:'class12',5:'class13',6:'class14',7:'class15',\r\n",
        "             8:'class16',9:'class17',10:'class18',11:'class19',12:'class2',13:'class3',\r\n",
        "             14:'class4',15:'class5',16:'class6',17:'class7',18:'class8',19:'class9'}\r\n",
        "\r\n",
        "sub = pd.DataFrame()\r\n",
        "sub[\"SEQUENCE_ID\"] = test_df[\"SEQUENCE_ID\"]\r\n",
        "# Adding test sequences to submission\r\n",
        "sub[\"SEQUENCE\"] = test_df[\"SEQUENCE\"]\r\n",
        "sub[\"LABEL\"] = y_classes\r\n",
        "sub['LABEL'].replace(idx2class, inplace=True)\r\n",
        "sub.head()\r\n",
        "sub.to_csv('drive/MyDrive/Enzyme/inputs/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfKtQYquVG7D"
      },
      "source": [
        "# open original training file\r\n",
        "# Concatenate training file with submission file\r\n",
        "\r\n",
        "train = pd.read_csv('drive/MyDrive/Enzyme/inputs/Train.csv')\r\n",
        "sub = pd.read_csv('drive/MyDrive/Enzyme/inputs/submission.csv')\r\n",
        "new_train = train.drop('CREATURE',axis=1)\r\n",
        "\r\n",
        "combined_csv = pd.concat([new_train,sub])\r\n",
        "#export to csv\r\n",
        "combined_csv.to_csv( \"drive/MyDrive/Enzyme/inputs/new_train.csv\", index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytFOHL3NhJf"
      },
      "source": [
        "# The new training file will go under the same procedure of creating folds, tokenization, and training \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}